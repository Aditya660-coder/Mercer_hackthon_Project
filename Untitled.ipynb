{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53248a6e",
   "metadata": {},
   "source": [
    "# USING CYCLOMATIC COMPLEXITY FOR PYTHON CODE COMPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c5abce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the GitHub profile URL: https://github.com/Aditya660-coder/GradientDescentMachineLearning\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "car_df=pd.read_csv('Car_Purchase_Data.csv',encoding='ISO-8859-1')\n",
      "car_df\n",
      "car_df.head(5)\n",
      "car_df.tail(5)\n",
      "\n",
      "sns.pairplot(car_df)\n",
      "x=car_df.drop(['Customer Name','Customer e-mail','Country','Car Purchase Amount'],axis=1)\n",
      "x\n",
      "y = car_df['Car Purchase Amount']\n",
      "y\n",
      "x.shape \n",
      "y.shape\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "x_scaled = scaler.fit_transform(x)\n",
      "x_scaled\n",
      "scaler.data_max_\n",
      "scaler.data_min_\n",
      "y=y.values.reshape(-1,1)\n",
      "y_scaled = scaler.fit_transform(y)\n",
      "y_scaled\n",
      "y_scaled.shape\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x_scaled,y_scaled,test_size=0.25)\n",
      "x_train.shape\n",
      "x_test.shape\n",
      "y_train.shape\n",
      "y_test.shape\n",
      "import tensorflow.keras \n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "model = Sequential()\n",
      "model.add(Dense(5,input_dim=5,activation='relu'))\n",
      "model.add(Dense(5,activation='relu'))\n",
      "model.add(Dense\n",
      "########################################\n",
      "'(' was never closed (<unknown>, line 39)\n",
      "Error processing file: Aditya660-coder/Car_Purchase_Machine_Learning_Model/Car_Purchase_Machine_Learning_model.ipynb - invalid literal for int() with base 10: \"'(' was never closed (<unknown>, line 39)\"\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from fbprophet import Prophet\n",
      "f1=pd.read_csv('chicago_crime_2014.csv',error_bad_lines=False)\n",
      "f2=pd.read_csv('chicago_crime_2015.csv',error_bad_lines=False)\n",
      "f3=pd.read_csv('chicago_crime_2016.csv',error_bad_lines=False)\n",
      "f=pd.concat([f1,f2,f3])\n",
      "f.head()\n",
      "f1.shape\n",
      "import seaborn as sns\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.heatmap(f.isnull(),cbar=False,cmap='Blues')\n",
      "f.head(3)\n",
      "f.drop(['ID','Case Number','IUCR','Beat','District','Ward','Community Area','FBI Code','Latitude','Longitude'],axis=1)\n",
      "f.Date=pd.to_datetime(f.Date,format='%m/%d/%Y %I:%M:%S %p')\n",
      "f.Date.head()\n",
      "f\n",
      "f['Primary Type'].value_counts()\n",
      "f['Primary Type'].value_counts().iloc[:15]\n",
      "order_data=f['Primary Type'].value_counts().iloc[:15].index\n",
      "order_data_Location_Description=f['Location Description'].value_counts().iloc[:15].index\n",
      "order_data_Location_Description\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.countplot(y='Primary Type',data=f,order=order_data)\n",
      "plt.figure(figsize=(15,1\n",
      "########################################\n",
      "'(' was never closed (<unknown>, line 26)\n",
      "Error processing file: Aditya660-coder/Chicago_crime_prediction_using_fbprophet-/Chicago_crime_prediction_using_fbprophet.ipynb - invalid literal for int() with base 10: \"'(' was never closed (<unknown>, line 26)\"\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"salary.csv\")\n",
      "df\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "lb =LabelEncoder()\n",
      "lb1=LabelEncoder()\n",
      "lb2 =LabelEncoder()\n",
      "df['company_n']=lb.fit_transform(df.company)\n",
      "df['job_n']=lb.fit_transform(df.job)\n",
      "df['degree_n']=lb.fit_transform(df.degree)\n",
      "df\n",
      "new_df = df.drop(['company','job','degree'],axis='columns')\n",
      "new_df\n",
      "pd.DataFrame(new_df)\n",
      "new_df.rename(columns={'salary_more_then_100k':'target'},inplace=True)\n",
      "from sklearn import tree\n",
      "model = tree.DecisionTreeClassifier()\n",
      "target=new_df['target']\n",
      "target\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test = train_test_split(new_df,target,test_size=0.3)\n",
      "x_train\n",
      "model.fit(x_train,y_train)\n",
      "import seaborn as sns\n",
      "from sklearn import metrics\n",
      "y_pred =model.predict(x_test)\n",
      "y_pred\n",
      "confusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
      "cm_display=metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix,display_labels=[False,True])\n",
      "import matplotlib.pyplot as plt\n",
      "cm_displa\n",
      "########################################\n",
      "0\n",
      "y.plot()\n",
      "plt.show()\n",
      "metrics.precision_score(y_pred,y_test)\n",
      "model.score(x_test,y_pred)\n",
      "\n",
      "########################################\n",
      "0\n",
      "import numpy as np\n",
      "x=[2,3,5,6,7]\n",
      "y =[26,45,25,36,56]\n",
      "def gradient_descent(x,y):\n",
      "    m_curr=0\n",
      "    b_curr=0\n",
      "    learning_rate=0.003\n",
      "    n = len(x)\n",
      "    itr=10000\n",
      "    for i in range(itr):\n",
      "        y_pred= m_curr* x + b_curr\n",
      "        md =-(2/n)*sum(x*(y-y_pred))\n",
      "        bd =-(1/n)*sum(y-y_pred)\n",
      "        m_curr= m_curr-(learning_rate*md)\n",
      "        b_curr=b_curr-(learning_rate*bd)\n",
      "        print(\"m_curr {}, b_curr {}, iteration {} \".format(m_curr,b_curr,i))\n",
      "    \n",
      "print(x)\n",
      "print(y)\n",
      "x = np.array(x)\n",
      "y = np.array(y)\n",
      "print(x,y)\n",
      "gradient_descent(x,y)\n",
      "\n",
      "########################################\n",
      "2\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "import matplotlib\n",
      "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
      "df1 = pd.read_csv(\"Bengaluru_House_Data.csv\")\n",
      "df1.head()\n",
      "df1.shape\n",
      "df1.groupby('area_type')['area_type'].agg('count')\n",
      "df2 = df1.drop(['area_type', 'society', 'balcony', 'availability'], axis = 'columns')\n",
      "df2.head()\n",
      "df2.isnull().sum() \n",
      "df3 = df2.dropna()\n",
      "df3.head()\n",
      "df3.shape\n",
      "df3['size'].unique()\n",
      "df3['bhk'] = df3['size'].apply(lambda x : int(x.split(' ')[0]))\n",
      "df3.head()\n",
      "df3[df3.bhk>20]\n",
      "df3.total_sqft.unique()\n",
      "def is_float(x):\n",
      "    try:\n",
      "        float(x)\n",
      "    except:\n",
      "        return False\n",
      "    return True\n",
      "df3[~df3['total_sqft'].apply(is_float)].head(10)\n",
      "def convert_sqft_to_num(x):\n",
      "    tokens = x.split('-')\n",
      "    if len(tokens) == 2:\n",
      "        return (float(tokens[0]) + float(tokens[1])) / 2\n",
      "    try:\n",
      "        return float(x)\n",
      "    except:\n",
      "        return None\n",
      "df4 = df3.copy()\n",
      "df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)\n",
      "df4.head(3)\n",
      "df5 = df4.c\n",
      "########################################\n",
      "2\n",
      "opy()\n",
      "df5['price_per_sqft'] = df5['price'] * 100000 / df5['total_sqft']\n",
      "len(df5.location.unique())\n",
      "df5.location = df5.location.apply(lambda x : x.strip())\n",
      "\n",
      "location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending = False)\n",
      "location_stats\n",
      "len(location_stats[location_stats<=10])\n",
      "location_stats_less_than_10 = location_stats[location_stats<=10]\n",
      "location_stats_less_than_10\n",
      "len(df5.location.unique())\n",
      "df5.location = df5.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)\n",
      "len(df5.location.unique())\n",
      "df5.head(10)\n",
      "df5[df5.total_sqft/df5.bhk<300].head()\n",
      "df5.shape\n",
      "df6 = df5[~(df5.total_sqft/df5.bhk<300)]\n",
      "df6.shape\n",
      "df6.price_per_sqft.describe()\n",
      "def remove_pps_outliers(df):\n",
      "    df_out = pd.DataFrame()\n",
      "    for key, subdf in df.groupby('location'):\n",
      "        m = np.mean(subdf.price_per_sqft)\n",
      "        st = np.std(subdf.price_per_sqft)\n",
      "        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n",
      "        df_out = pd.concat([\n",
      "########################################\n",
      "'[' was never closed (<unknown>, line 26)\n",
      "Error processing file: Aditya660-coder/House_price_prediction/main.ipynb - invalid literal for int() with base 10: \"'[' was never closed (<unknown>, line 26)\"\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import linear_model\n",
      "df = pd.read_csv(\"train.csv\")\n",
      "df\n",
      "df = df.head(6)\n",
      "df\n",
      "new_df = df[['area','bedrooms','age','price']]\n",
      "new_df\n",
      "\n",
      "import math\n",
      "median_bedrooms = math.floor(new_df.bedrooms.median())\n",
      "median_bedrooms\n",
      "new_df.bedrooms=new_df.bedrooms.fillna(median_bedrooms)\n",
      "new_df=new_df.astype({'bedrooms':'int','age':'int','price':'int'})\n",
      "new_df\n",
      "reg = linear_model.LinearRegression()\n",
      "reg.fit(new_df[['area','bedrooms','age']],new_df.price)\n",
      "reg.coef_\n",
      "\n",
      "reg.intercept_\n",
      "reg.predict([[3000,3,40]])\n",
      "\n",
      "########################################\n",
      "0\n",
      "import pandas as pd \n",
      "df = pd.read_csv(\"insurance.csv\")\n",
      "df\n",
      "x = df[['age']]\n",
      "y = df[['bought_insurance']]\n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(xtrain,ytrain)\n",
      "model.predict(xtest)\n",
      "ytest\n",
      "model.score(xtest,ytest)\n",
      "model.predict_proba(xtest)\n",
      "for i in range(len(model.predict_proba(xtest))):\n",
      "    \n",
      "    for j in range(len(model.predict_proba(xtest)[0])):\n",
      "        \n",
      "        print(model.predict_proba(xtest)[i][j])\n",
      "xtest\n",
      "\n",
      "########################################\n",
      "0\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"housepricing.csv\")\n",
      "df\n",
      "dummy = pd.get_dummies(df.town)\n",
      "dummy\n",
      "final = dummy.drop(['west windsor'],axis='columns')\n",
      "final\n",
      "merged = pd.concat([final,df],axis='columns')\n",
      "merged\n",
      "new_df = merged.drop(['town'],axis='columns')\n",
      "new_df\n",
      "x=new_df[['monroe township','robinsville','area']]\n",
      "x\n",
      "y = new_df.price\n",
      "y\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "model.predict([[1,0,2500]])[0]\n",
      "model.score(x,y)\n",
      "import pickle\n",
      "with open('model.pickle' ,'wb') as f:\n",
      "    pickle.dump(model,f)\n",
      "with open('model.pickle','rb') as f:\n",
      "    md = pickle.load(f)\n",
      "md.predict([[2,0,2500]])\n",
      "new_df \n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4)\n",
      "model.fit(x_train,y_train)\n",
      "model.score(x_test,y_test)\n",
      "model.coef_\n",
      "model.intercept_\n",
      "with open('model.pickle','wb') as f:\n",
      "    pickle.dump(model,f)\n",
      "\n",
      "########################################\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "df = pd.read_csv(\"housepricing.csv\")\n",
      "df\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "lb = LabelEncoder()\n",
      "dfle = df\n",
      "dfle.town =lb.fit_transform(dfle.town)\n",
      "dfle\n",
      "x=dfle[['town','area']]\n",
      "x\n",
      "y=dfle[['price']]\n",
      "y\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "model.predict([[0,2500]])[0]\n",
      "model.score(x,y)\n",
      "\n",
      "########################################\n",
      "0\n",
      "import pandas as pd \n",
      "df = pd.read_csv(\"insurance.csv\")\n",
      "df\n",
      "x = df[['age']]\n",
      "y = df[['bought_insurance']]\n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(xtrain,ytrain)\n",
      "model.predict(xtest)\n",
      "ytest\n",
      "model.score(xtest,ytest)\n",
      "model.predict_proba(xtest)\n",
      "for i in range(len(model.predict_proba(xtest))):\n",
      "    \n",
      "    for j in range(len(model.predict_proba(xtest)[0])):\n",
      "        \n",
      "        print(model.predict_proba(xtest)[i][j])\n",
      "xtest\n",
      "from sklearn.metrics import confusion_matrix\n",
      "y_pred=model.predict(xtest)\n",
      "y_pred\n",
      "from sklearn import metrics\n",
      "confusion_matrix = metrics.confusion_matrix(ytest, y_pred)\n",
      "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
      "import matplotlib.pyplot as plt\n",
      "cm_display.plot()\n",
      "plt.show()\n",
      "Precision = metrics.precision_score(y_pred, ytest)\n",
      "Precision\n",
      "\n",
      "########################################\n",
      "0\n",
      "They have some sort of order \n",
      "Example-Btech ,Mtech, Phd\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"hiring.csv\")\n",
      "df\n",
      "df['experience']=df['experience'].fillna(\"zero\")\n",
      "df\n",
      "pip install word2number\n",
      "from word2number import w2n\n",
      "df.experience=df.experience.apply(w2n.word_to_num)\n",
      "df\n",
      "import math\n",
      "df.rename(columns={'test_score(out of 10)': 'test_score'},inplace=True)\n",
      "df.rename(columns={'interview_score(out of 10)': 'interview_score'},inplace=True)\n",
      "df\n",
      "mean_value = math.floor(df.test_score.mean())\n",
      "print(mean_value)\n",
      "df.test_score=df.test_score.fillna(mean_value)\n",
      "df\n",
      "df.test_score=df.test_score.astype(int)\n",
      "df\n",
      "from sklearn import linear_model\n",
      "reg = linear_model.LinearRegression()\n",
      "reg.fit(df[['experience','test_score','interview_score']],df['salary($)'])\n",
      "reg.predict([[12,10,10]])[0]\n",
      "import pickle \n",
      "with open('model.pickle','wb') as f:\n",
      "    pickle.dump(reg,f)\n",
      "    \n",
      "with open('model.pickle','rb') as f:\n",
      "    model =pickle.load(f)\n",
      "model.predict([[10,6,5]])[0]\n",
      "\n",
      "########################################\n",
      "invalid syntax (<unknown>, line 1)\n",
      "Error processing file: Aditya660-coder/NiominalDataInMachineLearning/Untitled.ipynb - invalid literal for int() with base 10: 'invalid syntax (<unknown>, line 1)'\n",
      "import pandas as pd\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "dir(iris)\n",
      "df = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
      "df.head()\n",
      "df['target']=iris.target\n",
      "df\n",
      "df[df.target==0].head()\n",
      "df['flower_names']=df.target.apply([lambda x: iris.target_names[x]])\n",
      "df\n",
      "import matplotlib.pyplot as plt \n",
      "%matplotlib inline\n",
      "df =df.drop(['flower_names'],axis='columns')\n",
      "df0 = df[df.target==0]\n",
      "df1 = df[df.target==1]\n",
      "df2 = df[df.target==2]\n",
      "df\n",
      "new_df=df.target\n",
      "new_df.head()\n",
      "df2.head()\n",
      "x = df.drop(['target'],axis='columns')\n",
      "y = new_df\n",
      "x.head()\n",
      "plt.xlabel('sepal length (cm)')\n",
      "plt.ylabel('sepal width (cm)')\n",
      "plt.scatter(df0['sepal length (cm)'],df0['sepal width (cm)'],color='red',marker='*')\n",
      "plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],color='blue',marker='+')\n",
      "plt.scatter(df2['sepal length (cm)'],df2['sepal width (cm)'],color='yellow',marker='^')\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
      "from sklear\n",
      "########################################\n",
      "invalid syntax (<unknown>, line 13)\n",
      "Error processing file: Aditya660-coder/SVM/SVM.ipynb - invalid literal for int() with base 10: 'invalid syntax (<unknown>, line 13)'\n",
      "The most technically complex repository is: <a href='https://github.com/Aditya660-coder/GradientDescentMachineLearning'>Aditya660-coder/GradientDescentMachineLearning</a>\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import nbformat\n",
    "from github import Github\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer with the GPT-3 model's configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "# Define your GitHub access token\n",
    "github_access_token = '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'\n",
    "\n",
    "# Initialize the GitHub client\n",
    "g = Github(github_access_token)\n",
    "\n",
    "# Define your OpenAI API key\n",
    "openai_api_key = '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'\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Define a list of file extensions for code files (e.g., .py, .ipynb, .js, .php, .node)\n",
    "code_file_extensions = ['py', 'ipynb']\n",
    "\n",
    "# Define the maximum chunk size for code snippets\n",
    "max_chunk_size = 4000\n",
    "\n",
    "# Function to read and preprocess code from a file\n",
    "def preprocess_code(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            code = file.read()\n",
    "            # Add preprocessing steps here if needed (e.g., code cleaning)\n",
    "            return code\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Function to evaluate code complexity using GPT-3\n",
    "def evaluate_code_complexity(code):\n",
    "    # Adjust max tokens based on GPT-3's token limit (e.g., 4096 for GPT-3.5)\n",
    "    max_tokens = 2048\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=code,  # Pass the code directly as the prompt\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,  # Number of completions to generate\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"evaluate_code_complexity error\")\n",
    "        return str(e)\n",
    "import radon\n",
    "from radon.complexity import cc_visit\n",
    "\n",
    "def calculate_cyclomatic_complexity(code, max_size=1000):\n",
    "    # Check if the code size exceeds the maximum size limit\n",
    "    if len(code.splitlines()) > max_size:\n",
    "        return \"Code size exceeds the maximum limit for complexity analysis.\"\n",
    "\n",
    "    try:\n",
    "        results = cc_visit(code)\n",
    "        if results:\n",
    "            complexity = results[0].complexity\n",
    "            return complexity\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Function to split code into chunks and evaluate code complexity\n",
    "def evaluate_code_in_chunks(code):\n",
    "    max_chunk_size=1000\n",
    "    chunks = [code[i:i + max_chunk_size] for i in range(0, len(code), max_chunk_size)]\n",
    "\n",
    "    total_complexity_score = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n",
    "        print(\"########################################\")\n",
    "        complexity_score = calculate_cyclomatic_complexity(chunk)\n",
    "        print(complexity_score)\n",
    "        \n",
    "        total_complexity_score=int(total_complexity_score) + int(complexity_score)\n",
    "\n",
    "    return total_complexity_score\n",
    "\n",
    "# Function to fetch a user's repositories from GitHub using their username\n",
    "def fetch_user_repositories(github_username):\n",
    "    user = g.get_user(github_username)\n",
    "    return user.get_repos()\n",
    "\n",
    "# Function to identify the most technically complex repository\n",
    "def identify_most_complex_repo(repositories):\n",
    "    most_complex_repo = None\n",
    "    highest_complexity_score = float('-inf')\n",
    "\n",
    "    for repo in repositories:\n",
    "        # Create a directory with the repository name to store code files\n",
    "        repo_directory = repo.full_name.replace(\"/\", \"_\")\n",
    "        os.makedirs(repo_directory, exist_ok=True)\n",
    "\n",
    "        # Fetch code from code files in the repository\n",
    "        for content in repo.get_contents(\"\"):\n",
    "            if content.type == \"file\":\n",
    "                file_extension = content.name.split('.')[-1]\n",
    "\n",
    "                # Check if the file extension is in the list of code file extensions\n",
    "                if file_extension in code_file_extensions:\n",
    "                    try:\n",
    "                        decoded_content = content.decoded_content.decode('utf-8')\n",
    "                        preprocessed_content = decoded_content\n",
    "\n",
    "                        # Remove Python output lines (lines starting with \"Out[xx]:\") from Python files\n",
    "                        if file_extension == 'py' or file_extension == 'ipynb':\n",
    "                            code_lines = [line for line in preprocessed_content.splitlines() if not re.match(r'^\\s*Out\\[[0-9]+\\]:', line)]\n",
    "                        else:\n",
    "                            code_lines = preprocessed_content.splitlines()\n",
    "\n",
    "                        # Join the code lines and save to a file with the corresponding extension\n",
    "                        code_to_save = '\\n'.join(code_lines)\n",
    "                        file_path = os.path.join(repo_directory, content.name)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as code_file:\n",
    "                            code_file.write(code_to_save)\n",
    "\n",
    "                        # Specify the path to your Jupyter Notebook file (.ipynb)\n",
    "                        with open(file_path, 'r', encoding='utf-8') as notebook_file:\n",
    "                            notebook_content = nbformat.read(notebook_file, as_version=4)\n",
    "                            code_cells = []\n",
    "                            for cell in notebook_content.cells:\n",
    "                                if cell.cell_type == 'code':\n",
    "                                    code_lines = [line for line in cell.source.splitlines() if not line.strip().startswith('#')]\n",
    "                                    code_cells.append('\\n'.join(code_lines))\n",
    "                        code_to_evaluate = '\\n'.join(code_cells)\n",
    "\n",
    "                        # Evaluate code complexity in chunks\n",
    "                        complexity_score = evaluate_code_in_chunks(code_to_evaluate)\n",
    "\n",
    "                        if complexity_score > highest_complexity_score:\n",
    "                            highest_complexity_score = complexity_score\n",
    "                            most_complex_repo = repo\n",
    "\n",
    "                    except UnicodeDecodeError:\n",
    "                        # Handle encoding errors gracefully\n",
    "                        print(f\"Unable to decode file content: {repo.full_name}/{content.path} (It may be binary).\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {repo.full_name}/{content.path} - {str(e)}\")\n",
    "\n",
    "    return most_complex_repo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get GitHub profile URL as input\n",
    "    url = input(\"Enter the GitHub profile URL: \")\n",
    "\n",
    "    # Extract the GitHub username from the URL\n",
    "    match = re.search(r'https://github\\.com/([^/]+)', url)\n",
    "    if match:\n",
    "        github_username = match.group(1)\n",
    "    else:\n",
    "        print(\"Invalid GitHub profile URL.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Fetch user repositories\n",
    "    repositories = fetch_user_repositories(github_username)\n",
    "\n",
    "    # Identify the most technically complex repository\n",
    "    most_complex_repo = identify_most_complex_repo(repositories)\n",
    "\n",
    "    if most_complex_repo:\\\n",
    "        # Assuming you have identified the most_complex_repo earlier in your code\n",
    "        most_complex_repo_url = f\"https://github.com/{most_complex_repo.full_name}\"\n",
    "        message = f\"The most technically complex repository is: <a href='{most_complex_repo_url}'>{most_complex_repo.full_name}</a>\"\n",
    "        print(message)\n",
    "\n",
    "    else:\n",
    "        print(\"No repositories found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3a028",
   "metadata": {},
   "source": [
    "The most technically complex repository is: <a href='https://github.com/Aditya660-coder/GradientDescentMachineLearning'>Aditya660-coder/GradientDescentMachineLearning</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d667c99",
   "metadata": {},
   "source": [
    "It also has some error like syntax error which is occuring during dividing the text into chunk with maximum size of 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fdfa1",
   "metadata": {},
   "source": [
    "# USING GPT TOOL TO EVALUATE THE COMPLEXITY OF PYTHON CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b17059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the GitHub profile URL: https://github.com/Aditya660-coder\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "car_df=pd.read_csv('Car_Purchase_Data.csv',encoding='ISO-8859-1')\n",
      "car_df\n",
      "car_df.head(5)\n",
      "car_df.tail(5)\n",
      "\n",
      "sns.pairplot(car_df)\n",
      "x=car_df.drop(['Customer Name','Customer e-mail','Country','Car Purchase Amount'],axis=1)\n",
      "x\n",
      "y = car_df['Car Purchase Amount']\n",
      "y\n",
      "x.shape \n",
      "y.shape\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "x_scaled = scaler.fit_transform(x)\n",
      "x_scaled\n",
      "scaler.data_max_\n",
      "scaler.data_min_\n",
      "y=y.values.reshape(-1,1)\n",
      "y_scaled = scaler.fit_transform(y)\n",
      "y_scaled\n",
      "y_scaled.shape\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x_scaled,y_scaled,test_size=0.25)\n",
      "x_train.shape\n",
      "x_test.shape\n",
      "y_train.shape\n",
      "y_test.shape\n",
      "import tensorflow.keras \n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "model = Sequential()\n",
      "model.add(Dense(5,input_dim=5,activation='relu'))\n",
      "model.add(Dense(5,activation='relu'))\n",
      "model.add(Dense(1,activation='linear'))\n",
      "model.summary()\n",
      "model.compile(optimizer='adam',loss='mean_squared_error')\n",
      "epochs_hist = model.fit(x_train,y_train,epochs=100,batch_size=50,verbose=1,validation_split=0.2)\n",
      "epochs_hist.history.keys()\n",
      "plt.plot(epochs_hist.history['loss'])\n",
      "plt.plot(epochs_hist.history['val_loss'])\n",
      "plt.title('Model Loss Progress during training')\n",
      "plt.ylabel('training and validation loss')\n",
      "plt.xlabel('epoch number')\n",
      "plt.legend(['Training Loss','Validation Loss'])\n",
      "x_train=np.array([[1,40,20000,10000,500000]])\n",
      "y_predict=model.predict(x_train)\n",
      "print('Predicted car purchase amount',y_predict)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/Car_Purchase_Machine_Learning_Model/Car_Purchase_Machine_Learning_model.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from fbprophet import Prophet\n",
      "f1=pd.read_csv('chicago_crime_2014.csv',error_bad_lines=False)\n",
      "f2=pd.read_csv('chicago_crime_2015.csv',error_bad_lines=False)\n",
      "f3=pd.read_csv('chicago_crime_2016.csv',error_bad_lines=False)\n",
      "f=pd.concat([f1,f2,f3])\n",
      "f.head()\n",
      "f1.shape\n",
      "import seaborn as sns\n",
      "plt.figure(figsize=(10,10))\n",
      "sns.heatmap(f.isnull(),cbar=False,cmap='Blues')\n",
      "f.head(3)\n",
      "f.drop(['ID','Case Number','IUCR','Beat','District','Ward','Community Area','FBI Code','Latitude','Longitude'],axis=1)\n",
      "f.Date=pd.to_datetime(f.Date,format='%m/%d/%Y %I:%M:%S %p')\n",
      "f.Date.head()\n",
      "f\n",
      "f['Primary Type'].value_counts()\n",
      "f['Primary Type'].value_counts().iloc[:15]\n",
      "order_data=f['Primary Type'].value_counts().iloc[:15].index\n",
      "order_data_Location_Description=f['Location Description'].value_counts().iloc[:15].index\n",
      "order_data_Location_Description\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.countplot(y='Primary Type',data=f,order=order_data)\n",
      "plt.figure(figsize=(15,10))\n",
      "sns.countplot(y='Location Description',data=f,order=order_data_Location_Description)\n",
      "f.index=pd.DatetimeIndex(f.Date)\n",
      "f\n",
      "sample=f.resample('Y').size()\n",
      "plt.figure(figsize=(10,7))\n",
      "plt.plot(sample)\n",
      "plt.title('Crimes and Count Per Year')\n",
      "plt.xlabel('Years')\n",
      "plt.ylabel('Number of Crimes')\n",
      "plt.figure(figsize=(15,7))\n",
      "sampleM=f.resample('M').size()\n",
      "plt.plot(sampleM)\n",
      "plt.title('Crimes and Count Per Year')\n",
      "plt.xlabel('Months')\n",
      "plt.ylabel('Number of Crimes')\n",
      "chicago_f=f.resample('M').size().reset_index()\n",
      "chicago_f.head()\n",
      "chicago_f.columns=['Date','Crime_count']\n",
      "chicago_f\n",
      "chicago_f_final=chicago_f.rename(columns={'Date': 'ds','Crime_count':'y'})\n",
      "chicago_f_final\n",
      "m = Prophet()\n",
      "m.fit(chicago_f_final)\n",
      "future = m.make_future_dataframe(periods=730)\n",
      "forecast = m.predict(future)\n",
      "forecast\n",
      "figure = m.plot(forecast,xlabel='Date',ylabel='crime_rate')\n",
      "figure = m.plot_components(forecast)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/Chicago_crime_prediction_using_fbprophet-/Chicago_crime_prediction_using_fbprophet.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"salary.csv\")\n",
      "df\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "lb =LabelEncoder()\n",
      "lb1=LabelEncoder()\n",
      "lb2 =LabelEncoder()\n",
      "df['company_n']=lb.fit_transform(df.company)\n",
      "df['job_n']=lb.fit_transform(df.job)\n",
      "df['degree_n']=lb.fit_transform(df.degree)\n",
      "df\n",
      "new_df = df.drop(['company','job','degree'],axis='columns')\n",
      "new_df\n",
      "pd.DataFrame(new_df)\n",
      "new_df.rename(columns={'salary_more_then_100k':'target'},inplace=True)\n",
      "from sklearn import tree\n",
      "model = tree.DecisionTreeClassifier()\n",
      "target=new_df['target']\n",
      "target\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test = train_test_split(new_df,target,test_size=0.3)\n",
      "x_train\n",
      "model.fit(x_train,y_train)\n",
      "import seaborn as sns\n",
      "from sklearn import metrics\n",
      "y_pred =model.predict(x_test)\n",
      "y_pred\n",
      "confusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
      "cm_display=metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix,display_labels=[False,True])\n",
      "import matplotlib.pyplot as plt\n",
      "cm_display.plot()\n",
      "plt.show()\n",
      "metrics.precision_score(y_pred,y_test)\n",
      "model.score(x_test,y_pred)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/DecisionTreeClassifierAlgorithm/DecisionTreeClassifier.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import numpy as np\n",
      "x=[2,3,5,6,7]\n",
      "y =[26,45,25,36,56]\n",
      "def gradient_descent(x,y):\n",
      "    m_curr=0\n",
      "    b_curr=0\n",
      "    learning_rate=0.003\n",
      "    n = len(x)\n",
      "    itr=10000\n",
      "    for i in range(itr):\n",
      "        y_pred= m_curr* x + b_curr\n",
      "        md =-(2/n)*sum(x*(y-y_pred))\n",
      "        bd =-(1/n)*sum(y-y_pred)\n",
      "        m_curr= m_curr-(learning_rate*md)\n",
      "        b_curr=b_curr-(learning_rate*bd)\n",
      "        print(\"m_curr {}, b_curr {}, iteration {} \".format(m_curr,b_curr,i))\n",
      "    \n",
      "print(x)\n",
      "print(y)\n",
      "x = np.array(x)\n",
      "y = np.array(y)\n",
      "print(x,y)\n",
      "gradient_descent(x,y)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/GradientDescentMachineLearning/Gradient_descent_in_Machine_Learning.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from matplotlib import pyplot as plt\n",
      "import matplotlib\n",
      "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
      "df1 = pd.read_csv(\"Bengaluru_House_Data.csv\")\n",
      "df1.head()\n",
      "df1.shape\n",
      "df1.groupby('area_type')['area_type'].agg('count')\n",
      "df2 = df1.drop(['area_type', 'society', 'balcony', 'availability'], axis = 'columns')\n",
      "df2.head()\n",
      "df2.isnull().sum() \n",
      "df3 = df2.dropna()\n",
      "df3.head()\n",
      "df3.shape\n",
      "df3['size'].unique()\n",
      "df3['bhk'] = df3['size'].apply(lambda x : int(x.split(' ')[0]))\n",
      "df3.head()\n",
      "df3[df3.bhk>20]\n",
      "df3.total_sqft.unique()\n",
      "def is_float(x):\n",
      "    try:\n",
      "        float(x)\n",
      "    except:\n",
      "        return False\n",
      "    return True\n",
      "df3[~df3['total_sqft'].apply(is_float)].head(10)\n",
      "def convert_sqft_to_num(x):\n",
      "    tokens = x.split('-')\n",
      "    if len(tokens) == 2:\n",
      "        return (float(tokens[0]) + float(tokens[1])) / 2\n",
      "    try:\n",
      "        return float(x)\n",
      "    except:\n",
      "        return None\n",
      "df4 = df3.copy()\n",
      "df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)\n",
      "df4.head(3)\n",
      "df5 = df4.copy()\n",
      "df5['price_per_sqft'] = df5['price'] * 100000 / df5['total_sqft']\n",
      "len(df5.location.unique())\n",
      "df5.location = df5.location.apply(lambda x : x.strip())\n",
      "\n",
      "location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending = False)\n",
      "location_stats\n",
      "len(location_stats[location_stats<=10])\n",
      "location_stats_less_than_10 = location_stats[location_stats<=10]\n",
      "location_stats_less_than_10\n",
      "len(df5.location.unique())\n",
      "df5.location = df5.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)\n",
      "len(df5.location.unique())\n",
      "df5.head(10)\n",
      "df5[df5.total_sqft/df5.bhk<300].head()\n",
      "df5.shape\n",
      "df6 = df5[~(df5.total_sqft/df5.bhk<300)]\n",
      "df6.shape\n",
      "df6.price_per_sqft.describe()\n",
      "def remove_pps_outliers(df):\n",
      "    df_out = pd.DataFrame()\n",
      "    for key, subdf in df.groupby('location'):\n",
      "        m = np.mean(subdf.price_per_sqft)\n",
      "        st = np.std(subdf.price_per_sqft)\n",
      "        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]\n",
      "        df_out = pd.concat([df_out,reduced_df],ignore_index=True)\n",
      "    return df_out\n",
      "df7 = remove_pps_outliers(df6)\n",
      "df7.shape\n",
      "\n",
      "def plot_scatter_chart(df,location):\n",
      "    bhk2 = df[(df.location==location) & (df.bhk==2)]\n",
      "    bhk3 = df[(df.location==location) & (df.bhk==3)]\n",
      "    matplotlib.rcParams['figure.figsize'] = (15,10)\n",
      "    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK', s=50)\n",
      "    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='green',label='3 BHK', s=50)\n",
      "    plt.xlabel(\"Total Square Feet Area\")\n",
      "    plt.ylabel(\"Price (Lakh Indian Rupees)\")\n",
      "    plt.title(location)\n",
      "    plt.legend()\n",
      "    \n",
      "plot_scatter_chart(df7,\"Rajaji Nagar\")\n",
      "def remove_bhk_outliers(df):\n",
      "    exclude_indices = np.array([])\n",
      "    for location, location_df in df.groupby('location'):\n",
      "        bhk_stats = {}\n",
      "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
      "            bhk_stats[bhk] = {\n",
      "                'mean': np.mean(bhk_df.price_per_sqft),\n",
      "                'std': np.std(bhk_df.price_per_sqft),\n",
      "                'count': bhk_df.shape[0]\n",
      "            }\n",
      "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
      "            stats = bhk_stats.get(bhk-1)\n",
      "            if stats and stats['count']>5:\n",
      "                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n",
      "    return df.drop(exclude_indices,axis='index')\n",
      "df8 = remove_bhk_outliers(df7)\n",
      "df8.shape\n",
      "plot_scatter_chart(df8,\"Rajaji Nagar\")\n",
      "plot_scatter_chart(df8, \"Hebbal\")\n",
      "import matplotlib\n",
      "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
      "plt.hist(df8.price_per_sqft,rwidth=0.8)\n",
      "plt.xlabel(\"Price Per Square Feet\")\n",
      "plt.ylabel(\"Count\")\n",
      "df8.bath.unique()\n",
      "plt.hist(df8.bath,rwidth=0.8)\n",
      "plt.xlabel(\"Number of bathrooms\")\n",
      "plt.ylabel(\"Count\")\n",
      "df8[df8.bath>10]\n",
      "df8[df8.bath>df8.bhk+2]\n",
      "df9 = df8[df8.bath<df8.bhk+2]\n",
      "df9.shape\n",
      "df9.head(2)\n",
      "df10 = df9.drop(['size','price_per_sqft'],axis='columns')\n",
      "df10.head(3)\n",
      "dummies = pd.get_dummies(df10.location)\n",
      "dummies.head(3)\n",
      "df11 = pd.concat([df10,dummies.drop('other',axis='columns')],axis='\n",
      "########################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/House_price_prediction/main.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import linear_model\n",
      "df = pd.read_csv(\"train.csv\")\n",
      "df\n",
      "df = df.head(6)\n",
      "df\n",
      "new_df = df[['area','bedrooms','age','price']]\n",
      "new_df\n",
      "\n",
      "import math\n",
      "median_bedrooms = math.floor(new_df.bedrooms.median())\n",
      "median_bedrooms\n",
      "new_df.bedrooms=new_df.bedrooms.fillna(median_bedrooms)\n",
      "new_df=new_df.astype({'bedrooms':'int','age':'int','price':'int'})\n",
      "new_df\n",
      "reg = linear_model.LinearRegression()\n",
      "reg.fit(new_df[['area','bedrooms','age']],new_df.price)\n",
      "reg.coef_\n",
      "\n",
      "reg.intercept_\n",
      "reg.predict([[3000,3,40]])\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/LinearRegresionMultipleVariabl/LinearRegresionMultipleVariable.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd \n",
      "df = pd.read_csv(\"insurance.csv\")\n",
      "df\n",
      "x = df[['age']]\n",
      "y = df[['bought_insurance']]\n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(xtrain,ytrain)\n",
      "model.predict(xtest)\n",
      "ytest\n",
      "model.score(xtest,ytest)\n",
      "model.predict_proba(xtest)\n",
      "for i in range(len(model.predict_proba(xtest))):\n",
      "    \n",
      "    for j in range(len(model.predict_proba(xtest)[0])):\n",
      "        \n",
      "        print(model.predict_proba(xtest)[i][j])\n",
      "xtest\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/LogisticRegressionDummyVraiableLabelEncoder/LogisticregressionInMachinelearningSingleVariable.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"housepricing.csv\")\n",
      "df\n",
      "dummy = pd.get_dummies(df.town)\n",
      "dummy\n",
      "final = dummy.drop(['west windsor'],axis='columns')\n",
      "final\n",
      "merged = pd.concat([final,df],axis='columns')\n",
      "merged\n",
      "new_df = merged.drop(['town'],axis='columns')\n",
      "new_df\n",
      "x=new_df[['monroe township','robinsville','area']]\n",
      "x\n",
      "y = new_df.price\n",
      "y\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "model.predict([[1,0,2500]])[0]\n",
      "model.score(x,y)\n",
      "import pickle\n",
      "with open('model.pickle' ,'wb') as f:\n",
      "    pickle.dump(model,f)\n",
      "with open('model.pickle','rb') as f:\n",
      "    md = pickle.load(f)\n",
      "md.predict([[2,0,2500]])\n",
      "new_df \n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4)\n",
      "model.fit(x_train,y_train)\n",
      "model.score(x_test,y_test)\n",
      "model.coef_\n",
      "model.intercept_\n",
      "with open('model.pickle','wb') as f:\n",
      "    pickle.dump(model,f)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/LogisticRegressionDummyVraiableLabelEncoder/MachineLearningModelUsingDummyVariable.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"housepricing.csv\")\n",
      "df\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "lb = LabelEncoder()\n",
      "dfle = df\n",
      "dfle.town =lb.fit_transform(dfle.town)\n",
      "dfle\n",
      "x=dfle[['town','area']]\n",
      "x\n",
      "y=dfle[['price']]\n",
      "y\n",
      "from sklearn.linear_model import LinearRegression\n",
      "model = LinearRegression()\n",
      "model.fit(x,y)\n",
      "model.predict([[0,2500]])[0]\n",
      "model.score(x,y)\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/LogisticRegressionDummyVraiableLabelEncoder/ModelUsingLabelEncoderInMachineLearning.ipynb - 'NoneType' object has no attribute 'group'\n",
      "import pandas as pd \n",
      "df = pd.read_csv(\"insurance.csv\")\n",
      "df\n",
      "x = df[['age']]\n",
      "y = df[['bought_insurance']]\n",
      "x\n",
      "y\n",
      "from sklearn.model_selection import train_test_split\n",
      "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "model = LogisticRegression()\n",
      "model.fit(xtrain,ytrain)\n",
      "model.predict(xtest)\n",
      "ytest\n",
      "model.score(xtest,ytest)\n",
      "model.predict_proba(xtest)\n",
      "for i in range(len(model.predict_proba(xtest))):\n",
      "    \n",
      "    for j in range(len(model.predict_proba(xtest)[0])):\n",
      "        \n",
      "        print(model.predict_proba(xtest)[i][j])\n",
      "xtest\n",
      "from sklearn.metrics import confusion_matrix\n",
      "y_pred=model.predict(xtest)\n",
      "y_pred\n",
      "from sklearn import metrics\n",
      "confusion_matrix = metrics.confusion_matrix(ytest, y_pred)\n",
      "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
      "import matplotlib.pyplot as plt\n",
      "cm_display.plot()\n",
      "plt.show()\n",
      "Precision = metrics.precision_score(y_pred, ytest)\n",
      "Precision\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/LogisticregressionInMachinelearningSingleVariable/LogisticregressionInMachinelearningSingleVariable.ipynb - 'NoneType' object has no attribute 'group'\n",
      "They have some sort of order \n",
      "Example-Btech ,Mtech, Phd\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"hiring.csv\")\n",
      "df\n",
      "df['experience']=df['experience'].fillna(\"zero\")\n",
      "df\n",
      "pip install word2number\n",
      "from word2number import w2n\n",
      "df.experience=df.experience.apply(w2n.word_to_num)\n",
      "df\n",
      "import math\n",
      "df.rename(columns={'test_score(out of 10)': 'test_score'},inplace=True)\n",
      "df.rename(columns={'interview_score(out of 10)': 'interview_score'},inplace=True)\n",
      "df\n",
      "mean_value = math.floor(df.test_score.mean())\n",
      "print(mean_value)\n",
      "df.test_score=df.test_score.fillna(mean_value)\n",
      "df\n",
      "df.test_score=df.test_score.astype(int)\n",
      "df\n",
      "from sklearn import linear_model\n",
      "reg = linear_model.LinearRegression()\n",
      "reg.fit(df[['experience','test_score','interview_score']],df['salary($)'])\n",
      "reg.predict([[12,10,10]])[0]\n",
      "import pickle \n",
      "with open('model.pickle','wb') as f:\n",
      "    pickle.dump(reg,f)\n",
      "    \n",
      "with open('model.pickle','rb') as f:\n",
      "    model =pickle.load(f)\n",
      "model.predict([[10,6,5]])[0]\n",
      "\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/NiominalDataInMachineLearning/Untitled.ipynb - 'NoneType' object has no attribute 'group'\n",
      "Error processing file: Aditya660-coder/Spotify_clone-frontend-/script.js - Notebook does not appear to be JSON: 'console.log(\"Welcome to spotify\")\\n// i...\n",
      "import pandas as pd\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "dir(iris)\n",
      "df = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
      "df.head()\n",
      "df['target']=iris.target\n",
      "df\n",
      "df[df.target==0].head()\n",
      "df['flower_names']=df.target.apply([lambda x: iris.target_names[x]])\n",
      "df\n",
      "import matplotlib.pyplot as plt \n",
      "%matplotlib inline\n",
      "df =df.drop(['flower_names'],axis='columns')\n",
      "df0 = df[df.target==0]\n",
      "df1 = df[df.target==1]\n",
      "df2 = df[df.target==2]\n",
      "df\n",
      "new_df=df.target\n",
      "new_df.head()\n",
      "df2.head()\n",
      "x = df.drop(['target'],axis='columns')\n",
      "y = new_df\n",
      "x.head()\n",
      "plt.xlabel('sepal length (cm)')\n",
      "plt.ylabel('sepal width (cm)')\n",
      "plt.scatter(df0['sepal length (cm)'],df0['sepal width (cm)'],color='red',marker='*')\n",
      "plt.scatter(df1['sepal length (cm)'],df1['sepal width (cm)'],color='blue',marker='+')\n",
      "plt.scatter(df2['sepal length (cm)'],df2['sepal width (cm)'],color='yellow',marker='^')\n",
      "from sklearn.model_selection import train_test_split\n",
      "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
      "from sklearn.svm import SVC\n",
      "model = SVC()\n",
      "model.fit(x_train,y_train)\n",
      "model.score(x_test,y_test)\n",
      "y_pred= model.predict(x_test)\n",
      "########################################\n",
      "evaluate_code_complexity error\n",
      "Error processing file: Aditya660-coder/SVM/SVM.ipynb - 'NoneType' object has no attribute 'group'\n",
      "No repositories found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nbformat\n",
    "from github import Github\n",
    "from transformers import AutoTokenizer\n",
    "from radon.complexity import cc_visit\n",
    "\n",
    "# Initialize the tokenizer with the GPT-3 model's configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "# Define your GitHub access token\n",
    "github_access_token = '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'\n",
    "\n",
    "# Initialize the GitHub client\n",
    "g = Github(github_access_token)\n",
    "\n",
    "# Define your OpenAI API key\n",
    "openai_api_key = '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'\n",
    "\n",
    "# Initialize the OpenAI API client\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Define a list of file extensions for code files (e.g., .py, .ipynb, .js, .php, .node)\n",
    "code_file_extensions = ['py', 'ipynb', 'js', 'php', 'node']\n",
    "\n",
    "# Define the maximum chunk size for code snippets\n",
    "max_chunk_size = 4000\n",
    "\n",
    "# Function to read and preprocess code from a file\n",
    "def preprocess_code(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            code = file.read()\n",
    "            # Add preprocessing steps here if needed (e.g., code cleaning)\n",
    "            return code\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Function to evaluate code complexity using GPT-3\n",
    "def evaluate_code_complexity(code):\n",
    "    # Adjust max tokens based on GPT-3's token limit (e.g., 4096 for GPT-3.5)\n",
    "    max_tokens = 2048\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=code,  # Pass the code directly as the prompt\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,  # Number of completions to generate\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"evaluate_code_complexity error\")\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "# Function to split code into chunks and evaluate code complexity\n",
    "def evaluate_code_in_chunks(code):\n",
    "    chunks = [code[i:i + max_chunk_size] for i in range(0, len(code), max_chunk_size)]\n",
    "\n",
    "    total_complexity_score = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        print(chunk)\n",
    "        print(\"########################################\")\n",
    "        complexity_evaluation = evaluate_code_complexity(chunk)\n",
    "        complexity_score = int(re.search(r'\\d+', complexity_evaluation).group())\n",
    "        total_complexity_score += complexity_score\n",
    "\n",
    "    return total_complexity_score\n",
    "\n",
    "# Function to fetch a user's repositories from GitHub using their username\n",
    "def fetch_user_repositories(github_username):\n",
    "    user = g.get_user(github_username)\n",
    "    return user.get_repos()\n",
    "\n",
    "# Function to identify the most technically complex repository using Radon\n",
    "def identify_most_complex_repo_radon(repositories):\n",
    "    most_complex_repo = None\n",
    "    highest_complexity_score = 0  # Initialize with 0\n",
    "\n",
    "    for repo in repositories:\n",
    "        # Create a directory with the repository name to store code files\n",
    "        repo_directory = repo.full_name.replace(\"/\", \"_\")\n",
    "        os.makedirs(repo_directory, exist_ok=True)\n",
    "\n",
    "        # Fetch code from code files in the repository\n",
    "        for content in repo.get_contents(\"\"):\n",
    "            if content.type == \"file\":\n",
    "                file_extension = content.name.split('.')[-1]\n",
    "\n",
    "                # Check if the file extension is in the list of code file extensions\n",
    "                if file_extension in code_file_extensions:\n",
    "                    try:\n",
    "                        decoded_content = content.decoded_content.decode('utf-8')\n",
    "                        preprocessed_content = decoded_content\n",
    "\n",
    "                        # Remove Python output lines (lines starting with \"Out[xx]:\") from Python files\n",
    "                        if file_extension == 'py' or file_extension == 'ipynb':\n",
    "                            code_lines = [line for line in preprocessed_content.splitlines() if not re.match(r'^\\s*Out\\[[0-9]+\\]:', line)]\n",
    "                        else:\n",
    "                            code_lines = preprocessed_content.splitlines()\n",
    "\n",
    "                        # Join the code lines and save to a file with the corresponding extension\n",
    "                        code_to_save = '\\n'.join(code_lines)\n",
    "                        file_path = os.path.join(repo_directory, content.name)\n",
    "                        with open(file_path, 'w', encoding='utf-8') as code_file:\n",
    "                            code_file.write(code_to_save)\n",
    "\n",
    "                        # Specify the path to your Jupyter Notebook file (.ipynb)\n",
    "                        with open(file_path, 'r', encoding='utf-8') as notebook_file:\n",
    "                            notebook_content = nbformat.read(notebook_file, as_version=4)\n",
    "                            code_cells = []\n",
    "                            for cell in notebook_content.cells:\n",
    "                                if cell.cell_type == 'code':\n",
    "                                    code_lines = [line for line in cell.source.splitlines() if not line.strip().startswith('#')]\n",
    "                                    code_cells.append('\\n'.join(code_lines))\n",
    "                        code_to_evaluate = '\\n'.join(code_cells)\n",
    "\n",
    "                        # Evaluate code complexity in chunks using Radon\n",
    "                        complexity_score = evaluate_code_in_chunks(code_to_evaluate)\n",
    "\n",
    "                        if complexity_score > highest_complexity_score:\n",
    "                            highest_complexity_score = complexity_score\n",
    "                            most_complex_repo = repo\n",
    "\n",
    "                    except UnicodeDecodeError:\n",
    "                        # Handle encoding errors gracefully\n",
    "                        print(f\"Unable to decode file content: {repo.full_name}/{content.path} (It may be binary).\\n\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file: {repo.full_name}/{content.path} - {str(e)}\")\n",
    "\n",
    "    return most_complex_repo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get GitHub profile URL as input\n",
    "    url = input(\"Enter the GitHub profile URL: \")\n",
    "\n",
    "    # Extract the GitHub username from the URL\n",
    "    match = re.search(r'https://github\\.com/([^/]+)', url)\n",
    "    if match:\n",
    "        github_username = match.group(1)\n",
    "    else:\n",
    "        print(\"Invalid GitHub profile URL.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Fetch user repositories\n",
    "    repositories = fetch_user_repositories(github_username)\n",
    "\n",
    "    # Identify the most technically complex repository using Radon\n",
    "    most_complex_repo = identify_most_complex_repo_radon(repositories)\n",
    "\n",
    "    if most_complex_repo:\n",
    "        print(f\"The most technically complex repository is: {most_complex_repo.full_name}\")\n",
    "    else:\n",
    "        print(\"No repositories found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242988e",
   "metadata": {},
   "source": [
    "evaluate_code_complexity error\n",
    "Error processing file: Aditya660-coder/SVM/SVM.ipynb - 'NoneType' object has no attribute 'group'\n",
    "No repositories found.It is giving error because I don't have subscription of openai.com Or may be another error can be occured"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
